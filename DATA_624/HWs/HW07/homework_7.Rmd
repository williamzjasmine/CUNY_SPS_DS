---
title: "Data 624 - HW 7 - Linear Regression"
author: "William Jasmine"
date: "2024-10-28"
output:
  html_document: default
  pdf_document: default
---

All exercises can be found in ["Applied Predictive Modelling"](http://appliedpredictivemodeling.com/) written by Max Kuhn and Kjell Johnson.

# Setup

The following packages are required to rerun this `.rmd` file:

```{r setup, error=FALSE, warning=FALSE, message=FALSE}
seed_num <- 200
library(tidyverse)
library(AppliedPredictiveModeling)
library(caret)
library(pls)
library(elasticnet)
library(Hmisc)
```

# Exercise 6.2
## Description

Developing a model to predict permeability (see Sect. 1.4.) could save significant resources for a pharmaceutical company, while at the same time more rapidly identifying molecules that have a sufficient permeability to become a drug. 

## Part (a)

Start R and use these commands to load the data:

```{r}
data(permeability)
```

The matrix `fingerprints` contains the 1,107 binary molecular predictors for the 165 compounds, while `permeatbility` contains permeability response. 

## Part (b)
### Description

The fingerprint predictors indicate the presence or absence of substructures of a molecule and are often sparse meaning that relatively few of the molecules contain each substructure. Filter out the predictors that have low frequencies using the `nearZeroVar` function from the `caret` package. How many predictors are left for modelling?

### Solution 

The cell below uses the `nearZeroVar()` function to remove all of the sparse predictors from the `fingerprints` matrix`: 

```{r}
X <- fingerprints[,-nearZeroVar(fingerprints)]
dim(X)
```

The `dim` output shows that only 388 of our original 1,107 predictors remain after this transformation. 

## Part (c)

### Description

Split the data into a training and a test set, pre-process the data, and tune a `PLS` model. How many latent variables are optimal and what is the corresponding re-sampled estimate of $R^2$?

### Solution 

First, the cell below splits the data into a training and test set, with 70% of the observations being used for training:

```{r}
set.seed(seed_num)

train_ratio <- 0.7
n <- nrow(X)
n_train <- round(train_ratio * n)

train_ind <- sample(1:n, n_train)

all_data <- cbind(X, permeability)

train <- all_data[train_ind,]
test <- all_data[-train_ind,]
```

The cell below handles the pre-processing of the data, ensuring that each column of the training data is centered and scaled (each column will have `\mu=0` and `\sigma=1`. The same scaling parameters that were used on the training data are also applied to the test set data to guarantee consistency and prevent data leakage. 

```{r}
# center and scale the test data
train <- scale(train)

# get the scaling and centering params for each column
center_params <- attr(train, "scaled:center")
scaling_params <- attr(train, "scaled:scale")

# apply those parameters to the test set
test <- scale(test, center = center_params, scale = scaling_params)

# split into X and y sets 
X_train <- train[, -ncol(train)]
y_train <- train[, ncol(train)]

X_test <- test[, -ncol(test)]
y_test <- test[, ncol(test)]
```

Finally, we can build a tuned partial least squares (PLS regression model):

```{r}
set.seed(seed_num)

# perform cross-validation with 10 folds
ctrl <- trainControl(method = "cv", number = 10)

pls <- train(X_train, y_train,
             method = 'pls',
             tuneLength = 20,
             trControl = ctrl
)

pls
```

```{r}
plot(pls)
```

The plot makes it clear that a model with 7 latent variables (`ncomp=7`) was the best choice for minimizing the root mean squared error. Based on the output from the tuning, the re-sampling estimate of $R^2$ using 7 latent variables is $\approx 0.492$.

## Part (d)
### Description

Predict the response for the test set. What is the test set estimate of $R^2$?

### Solution

The cell below calculates $R^2$ using the tuned PLS model predictions on the test set data: 

```{r}
preds <- predict(pls, X_test)
defaultSummary(data.frame(obs = y_test, pred = preds))
```

We see that the $R^2$ value is slightly lower than the maximum $R^2$ achieved using the training set, which is expected. 

## Part (e)
### Description

Try building other methods discussed in this chapter. Do any have better predictive performance?

### Solution 

The steps outlined in parts (c)-(e) are repeated below using a ridge regression model, which is a type of penalized regression model: 

```{r, warning=FALSE}
ridge_grid <- data.frame(.lambda = seq(0, .1, length = 15))

ridge <- train(X_train, y_train,
               method = 'ridge',
               tuneGrid = ridge_grid,
               trControl = ctrl
)

preds <- predict(ridge, X_test)
defaultSummary(data.frame(obs = y_test, pred = preds))
```

Elastic net models are also an option, which combine the penalties from the ridge and lasso regression models: 

```{r, warning=FALSE}
enet_grid <- expand.grid(.lambda = c(0, 0.01, .1),
                          .fraction = seq(.05, 1, length = 20))

enet <- train(X_train, y_train,
               method = 'enet',
               tuneGrid = enet_grid,
               trControl = ctrl
)

preds <- predict(enet, X_test)
defaultSummary(data.frame(obs = y_test, pred = preds))
```

Lastly, the cell below implements good old fashioned ordinary linear regression:

```{r}
olr <- lm(permeability ~ ., data=data.frame(train))
preds <- predict(olr, data.frame(X_test))
defaultSummary(data.frame(obs = y_test, pred = preds))
```

The outputs of the penalized regression models (ridge and lasso) did not perform much differently than the PLS model implemented earlier, but these three showed significant improvement compared to the ordinary linear regression model (which only achieved an $R^2$ of $\approx 0.14$ on the test set data).

## Part (f)
### Description

Would you recommend any of your models to replace the permeability laboratory experiment?

### Solution 

While the PLS or penalized regression models explored in the previous parts of this problem did exhibit some degree of predictive power, none of them were able to achieve an $R^2$ value greater than 0.5. This is typically not considered a impressive model performance and thus should not completely replace the current methods in place to examine molecule permeability. That being said, the model could be used as a way to help limit the number of molecules examined, and should continued to be explored as more/better data continues to be gathered. 

# Exercise 6.3
## Description 

A chemical manufacturing process for a pharmaceutical product was discussed in Sect. 1.4. In this problem, the objective is to understand the relationship between biological measurements of the raw materials (predictors), measurements of the manufacturing process (predictors), and the response of product yield. Biological predictions cannot be changed but can be used to assess the vitality of the raw material before processing. On the other hand, manufacturing process predictors can be changed in the manufacturing process. Improving product yield by 1% will boost revenue by approximately one hundred thousand dollars per batch.

## Part (a)

Start R and use these commands to load the data: 

```{r}
data(ChemicalManufacturingProcess)
```

The matrix `processPredictors` contains the 57 predictors (12 describing the input biological material and 45 describing the process predictors) for the 176 manufacturing runs. `yield` contains the percent yield for each run. 

## Part (b)
### Description

A small percentage of cells in the predictor set contain missing values. Use an imputation function to fill in these missing values.

### Solution

The cell below imputes missing values using the median value from each missing value's respective column:

```{r}
imputation <- ChemicalManufacturingProcess %>%
  preProcess("medianImpute")

df <- predict(imputation, ChemicalManufacturingProcess)
any(is.na(df))
```

The output above proves that all missing values have now been filled.

## Part (c)
### Description

Split the data into a training and a test set, pre-process the data, and tune a model of your choice from this chapter. What is the optimal value of the performance metric? 

### Solution 

The cell below splits the data into training and testing sets:

```{r}
set.seed(seed_num)

train_ratio <- 0.7
n <- nrow(df)
n_train <- round(train_ratio * n)

train_ind <- sample(1:n, n_train)

train <- data.frame(df[train_ind,])
test <- data.frame(df[-train_ind,])
```

Next the data is pre-processed using the same methodology implemented in the solution for Exercise 6.2 Part (c):

```{r}
# center and scale the test data
train <- scale(train)

# get the scaling and centering params for each column
center_params <- attr(train, "scaled:center")
scaling_params <- attr(train, "scaled:scale")

# apply those parameters to the test set
test <- scale(test, center = center_params, scale = scaling_params)

# split into X and y sets 
X_train <- train[, -1]
y_train <- train[, 1]

X_test <- test[, -1]
y_test <- test[, 1]

```

Finally, the cell below tunes an elastic net regression model and plots the results of the tuning process: 

```{r}
enet_grid <- expand.grid(.lambda = c(0, 0.01, .1),
                          .fraction = seq(.05, 1, length = 20))

enet <- train(X_train, y_train,
               method = 'enet',
               tuneGrid = enet_grid,
               trControl = ctrl
)

final_model <- enet$finalModel
plot(enet)
```

The plot reveals that the optimal values of parameters are when the regularization strength (AKA $\lambda$ AKA the weight decay) is equal to 0.01, and when the `fraction` parameter is equal to 0.4. The performance of each examined model is also shown in the table below:

```{r}
enet
```

The optimal model in this case has an $R^2$ value of approximately 0.68 exhibiting a minimum RMSE of 0.57.

## Part (d)
### Description 

Predict the response for the test set. What is the value of the performance metric and how does it compare with the re-sampled performance metric on the training set?

### Solution

```{r}
preds <- predict(enet, X_test)
defaultSummary(data.frame(obs = y_test, pred = preds))
```

The model's performance clearly drops when calculating performance metrics using the test set. The $R^2$ dropped significantly, while the RMSE rose. Part of the reason for the significant decrease could simply be due to the small sample size of the test set.  

## Part (e)
### Description 

Which predictors are most important in the model you have trained? Do either biological or process predictors dominate the list?

### Solution 

Using the `varImp()` function, feature importances can be extracted from the tuned `enet` produced previously. The cell below calculates these feature importances and plots the top 20:

```{r}
var_imps <- data.frame(varImp(enet)$importance)
var_imps$predictor_name <- rownames(var_imps)
rownames(var_imps) <- NULL

top_imps <- var_imps %>%
  arrange(desc(Overall)) %>%
    slice_head(n = 20)

ggplot(top_imps, aes(x = reorder(predictor_name, -Overall), y = Overall)) +
  geom_bar(stat = "identity") +
  coord_flip() +  # Flip axes for better readability
  labs(
    title = "Variable Importance",
    x = "Variables",
    y = "Importance Score"
  ) +
  theme_minimal()
```

The plot reveals that both manufacturing and biological predictors were important to the model; neither seems to particularly "dominate" the list of top predictors. 

```{r}
man_imps <- var_imps %>%
  filter(startsWith(predictor_name, "Manufacturing"))

bio_imps <- var_imps %>%
  filter(startsWith(predictor_name, "Biological"))

avg_man_imp <- round(mean(man_imps$Overall), 2)
avg_bio_imp <- round(mean(bio_imps$Overall), 2)

print1 <- paste("Manufacturing average feature importance:", avg_man_imp)
print2 <- paste("Biological average feature importance:", avg_bio_imp)

cat(paste(print1, print2, sep='\n'))

```

However, the above output shows that the average importance of the biological predictors was about double that of the manufacturing predictors. This could hint that, overall, the biological features offer slightly superior predictive insight. 

## Part (f)

### Description 

Explore the relationships between each of the top predictors and the response. How could this information be helpful in improving yield in future runs of the manufacturing process?

### Solution

To explore the relationship between the top predictors and `yield`, the cell below determines the correlation of the top 20 predictors with the response variable: 

```{r}
top_names <- top_imps$predictor_name
rs <- c()

i <- 1
for(column in top_names){
  r_val <- cor(df[[column]], df[["Yield"]])
  rs[i] <- r_val
  i = i + 1
}

rs <- data.frame(predictor_name = top_names, r_value = rs)

ggplot(rs, aes(x = predictor_name, y = r_value)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Top Predictors Correlation with Yield",
    x = "Variables",
    y = "Pearson-r Correlation"
  ) +
  coord_flip() +
  theme_minimal()
```

The plot reveals that all of the top biological predictors have a positive correlation with `yield`, meaning that we can conclude that larger values of these predictors result in increased yield. The behavior of the top manufacturing predictors is less clear, with both positive and negative correlations being visible. That being said, these correlations can help paint a picture of how `yield` will be impacted should their value change in any way. 

