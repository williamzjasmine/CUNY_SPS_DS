{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "83708667-4fdc-1563-7b3a-06b6575d2865"
   },
   "source": [
    "# Pairs Trading - finding pairs based on Clustering\n",
    "\n",
    "The code here has been adapted from [O'Rielly's Machine Learning & Data Science Blueprints for Finance](https://github.com/tatsath/fin-ml/blob/bc10f73984d23e1e968b63c4066228d603ca02fb/Chapter%208%20-%20Unsup.%20Learning%20-%20Clustering/Case%20Study1%20-%20Clustering%20for%20Pairs%20Trading/ClusteringForPairsTrading.ipynb) online resources, and follows their outline to use clustering in identifying possible pairs-trading opportunities from this [investment portfolio](https://docs.google.com/spreadsheets/d/1EZj5M7dXGy-48i0PydZQa5gpUOYCQHDFRBlp_rU1sdo/edit#gid=1736594093)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Libraries and Dataset](#1)\n",
    "    * [2.1. Load Libraries](#1.1)    \n",
    "    * [2.2. Load Dataset](#1.2)\n",
    "* [3. Exploratory Data Analysis](#2)\n",
    "    * [3.1 Descriptive Statistics](#2.1)    \n",
    "    * [3.2. Data Visualisation](#2.2)\n",
    "* [4. Data Preparation](#3)\n",
    "    * [4.1 Data Cleaning](#3.1)\n",
    "    * [4.3.Data Transformation](#3.2)  \n",
    "* [5.Evaluate Algorithms and Models](#5)        \n",
    "    * [5.1. k-Means Clustering](#5.1)\n",
    "        * [5.1.1 Finding right number of clusters](#5.1.1)\n",
    "        * [5.1.2 Clustering and Visualization](#5.1.2)\n",
    "    * [5.2. Hierarchial Clustering (Agglomerative Clustering)](#5.2)\n",
    "        * [5.2.1. Building Hierarchy Graph/ Dendogram](#5.2.1)\n",
    "        * [5.2.2. Clustering and Visualization](#5.2.1) \n",
    "    * [5.3. Affinity Propagation Clustering](#5.3)\n",
    "        * [5.3.1 Visualising the cluster](#5.2.1)\n",
    "    * [5.4. Cluster Evaluation](#5.4)        \n",
    "* [6.Pair Selection](#6)        \n",
    "    * [6.1 Cointegration and Pair Selection Function](#6.1)    \n",
    "    * [6.2. Pair Visualization](#6.2)   \n",
    "* [7.Implement a Pairs Trading Methodology](#7)\n",
    "* [8.Conclusion](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "# 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in this case study is to perform clustering analysis on the stocks of the aforementioned investment portfolo and identify possible pairs trading opportunities. \n",
    "\n",
    "\n",
    "The data of the stocks is obtained using the `yfinance` package and a `.csv` version of the investment portfolio, which is imported below. Data for this analysis includes closing prices for all of the portfolio's assets three years prior to 2023-12-17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# 2. Getting Started- Loading the data and python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Loading the python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5d8fee34-f454-2642-8b06-ed719f0317e1"
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv, set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "import pandas_datareader as dr\n",
    "import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "\n",
    "#Import Model Packages \n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering,AffinityPropagation, DBSCAN\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn import cluster, covariance, manifold\n",
    "\n",
    "\n",
    "#Other Helper Packages and functions\n",
    "import matplotlib.ticker as ticker\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "## 2.2. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data already obtained from yahoo finance is imported.\n",
    "# dataset = read_csv('SP500Data.csv',index_col=0)\n",
    "\n",
    "investments = pd.read_csv('../Portfolio/investments.csv')\n",
    "investments['weights'] = investments['initial_investment'] / investments['initial_investment'].sum()\n",
    "portfolio_start = investments.initial_investment.sum()\n",
    "\n",
    "tickers = list(investments['ticker'].values)\n",
    "end_date = dt.date(2023,12,16)\n",
    "start_date = end_date - relativedelta(years=3)\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "# have to make separate dataframes because crypto markets don't close\n",
    "dataset_crypto = pd.DataFrame()\n",
    "\n",
    "for index, row in investments.iterrows():\n",
    "    ticker = row['ticker']\n",
    "    tmp = yf.Ticker(ticker)\n",
    "    tmp = tmp.history(start=start_date, end=end_date)['Close']\n",
    "    if row['investment_type'] == 'crypto':\n",
    "        dataset_crypto[ticker] = tmp\n",
    "    else:\n",
    "        dataset[ticker] = tmp\n",
    "\n",
    "        \n",
    "dataset = dataset.reset_index()\n",
    "dataset['Date'] = pd.to_datetime(dataset['Date']).dt.date\n",
    "dataset = dataset.set_index('Date')\n",
    "\n",
    "# need to remove times since they \"close\" at different times\n",
    "dataset_crypto = dataset_crypto.reset_index()\n",
    "dataset_crypto['Date'] = pd.to_datetime(dataset_crypto['Date']).dt.date\n",
    "dataset_crypto = dataset_crypto.set_index('Date')\n",
    "\n",
    "dataset = dataset.join(dataset_crypto)\n",
    "# only include stocks in final dataset\n",
    "dataset = dataset.drop(columns=['BNDX', 'GOVT', 'LQD', 'BTC-USD', 'ETH-USD'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "df6a4523-b385-69ee-c933-592826d81431"
   },
   "source": [
    "<a id='2'></a>\n",
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "## 3.1. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "52f85dc2-0f91-3c50-400e-ddc38bea966b"
   },
   "outputs": [],
   "source": [
    "# shape\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek at data\n",
    "set_option('display.width', 100)\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7bffeec0-5bbc-fffb-18f2-3da56b862ca3"
   },
   "outputs": [],
   "source": [
    "# describe data\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "# 4. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "## 4.1. Data Cleaning\n",
    "We check for the NAs in the rows, either drop them or fill them with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for any null values and removing the null values'''\n",
    "print('Null Values =',dataset.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting rid of the columns with more than 30% missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_fractions = dataset.isnull().mean().sort_values(ascending=False)\n",
    "\n",
    "missing_fractions.head(10)\n",
    "\n",
    "drop_list = sorted(list(missing_fractions[missing_fractions > 0.3].index))\n",
    "\n",
    "dataset.drop(labels=drop_list, axis=1, inplace=True)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are null values drop the rown contianing the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values with the last value available in the dataset. \n",
    "dataset=dataset.fillna(method='ffill')\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>\n",
    "## 4.2. Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of clustering, we will be using annual\n",
    "returns and variance as the variables as they are the indicators of the stock performance and its volatility. Let us prepare the return and volatility variables from the\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average annual percentage return and volatilities over a theoretical one year period\n",
    "returns = dataset.pct_change().mean() * 252\n",
    "returns = pd.DataFrame(returns)\n",
    "returns.columns = ['Returns']\n",
    "returns['Volatility'] = dataset.pct_change().std() * np.sqrt(252)\n",
    "data=returns\n",
    "#format the data as a numpy array to feed into the K-Means algorithm\n",
    "#data = np.asarray([np.asarray(returns['Returns']),np.asarray(returns['Volatility'])]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the variables should be on the same scale before applying clustering, otherwise a feature with large values will dominate the result. We use StandardScaler in sklearn to standardize the dataset’s features onto unit scale (mean = 0 and variance = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(data)\n",
    "rescaledDataset = pd.DataFrame(scaler.fit_transform(data),columns = data.columns, index = data.index)\n",
    "# summarize transformed data\n",
    "rescaledDataset.head(2)\n",
    "X=rescaledDataset\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters to clusters are the indices and the variables used in the clustering are the columns. Hence the data is in the right format to be fed to the clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "# 5. Evaluate Algorithms and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the following models:\n",
    "\n",
    "1. KMeans\n",
    "2. Hierarchical Clustering (Agglomerative Clustering)\n",
    "3. Affinity Propagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1'></a>\n",
    "## 5.1. K-Means Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1.1'></a>\n",
    "### 5.1.1. Finding optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we look at the following metrices:\n",
    "\n",
    "1. Sum of square errors (SSE) within clusters\n",
    "2. Silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distorsions = []\n",
    "max_loop=20\n",
    "for k in range(2, max_loop):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(X)\n",
    "    distorsions.append(kmeans.inertia_)\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(range(2, max_loop), distorsions)\n",
    "plt.xticks([i for i in range(2, max_loop)], rotation=75)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the sum of squared errors chart, it appears the elbow “kink” occurs 5 or 6\n",
    "clusters for this data. Certainly, we can see that as the number of clusters increase pass\n",
    "6, the sum of square of errors within clusters plateaus off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "silhouette_score = []\n",
    "for k in range(2, max_loop):\n",
    "        kmeans = KMeans(n_clusters=k,  random_state=10, n_init=10)\n",
    "        kmeans.fit(X)        \n",
    "        silhouette_score.append(metrics.silhouette_score(X, kmeans.labels_, random_state=10))\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(range(2, max_loop), silhouette_score)\n",
    "plt.xticks([i for i in range(2, max_loop)], rotation=75)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the silhouette score chart, we can see that there are various parts of the graph\n",
    "where a kink can be seen. Since there is not much a difference in SSE after 7 clusters,\n",
    "we would prefer 7 clusters in the K-means model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1.2'></a>\n",
    "### 5.1.2.  Clustering and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build the k-means model with six clusters and\n",
    "visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclust=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit with k-means\n",
    "k_means = cluster.KMeans(n_clusters=nclust)\n",
    "k_means.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting labels \n",
    "target_labels = k_means.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing how your clusters are formed is no easy task when the number of variables/dimensions in your dataset is very large. One of the methods of visualising a cluster in two-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = k_means.cluster_centers_\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(X.iloc[:,0],X.iloc[:,1], c = k_means.labels_, cmap =\"rainbow\", label = X.index)\n",
    "ax.set_title('k-Means results')\n",
    "ax.set_xlabel('Mean Return')\n",
    "ax.set_ylabel('Volatility')\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "plt.plot(centroids[:,0],centroids[:,1],'sg',markersize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the elements of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show number of stocks in each cluster\n",
    "clustered_series = pd.Series(index=X.index, data=k_means.labels_.flatten())\n",
    "# clustered stock with its cluster label\n",
    "clustered_series_all = pd.Series(index=X.index, data=k_means.labels_.flatten())\n",
    "clustered_series = clustered_series[clustered_series != -1]\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.barh(\n",
    "    range(len(clustered_series.value_counts())), # cluster labels, y axis\n",
    "    clustered_series.value_counts()\n",
    ")\n",
    "plt.title('Cluster Member Counts')\n",
    "plt.xlabel('Stocks in Cluster')\n",
    "plt.ylabel('Cluster Number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of stocks in a cluster range from 1 to 10. Although, the distribution is not equal, a good number of clusters with a significant number of stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2'></a>\n",
    "## 5.2. Hierarchical Clustering (Agglomerative Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step we look at the hierarchy graph and check for the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.1'></a>\n",
    "### 5.2.1. Building Hierarchy Graph/ Dendogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hierarchy class has a dendrogram method which takes the value returned by the linkage method of the same class. The linkage method takes the dataset and the method to minimize distances as parameters. We use 'ward' as the method since it minimizes then variants of distances between the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, ward\n",
    "\n",
    "#Calulate linkage\n",
    "Z= linkage(X, method='ward')\n",
    "Z[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to visualize an agglomerate clustering algorithm is through a dendogram, which displays a cluster tree, the leaves being the individual stocks and the root being the final single cluster. The \"distance\" between each cluster is shown on the y-axis, and thus the longer the branches are, the less correlated two clusters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Dendogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Stocks Dendrograms\")\n",
    "dendrogram(Z,labels = X.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once one big cluster is formed, the longest vertical distance without any horizontal line passing through it is selected and a horizontal line is drawn through it. The number of vertical lines this newly created horizontal line passes is equal to number of clusters.\n",
    "Then we select the distance threshold to cut the dendrogram to obtain the selected clustering level. The output is the cluster labelled for each row of data. As expected from the dendrogram, a cut at 5 gives us two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_threshold = 5\n",
    "clusters = fcluster(Z, distance_threshold, criterion='distance')\n",
    "chosen_clusters = pd.DataFrame(data=clusters, columns=['cluster'])\n",
    "chosen_clusters['cluster'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2.2'></a>\n",
    "### 5.2.2.  Clustering and Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclust = 4\n",
    "hc = AgglomerativeClustering(n_clusters=nclust, affinity = 'euclidean', linkage = 'ward')\n",
    "clust_labels1 = hc.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,10))\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(X.iloc[:,0],X.iloc[:,1], c =clust_labels1, cmap =\"rainbow\")\n",
    "ax.set_title('Hierarchical Clustering')\n",
    "ax.set_xlabel('Mean Return')\n",
    "ax.set_ylabel('Volatility')\n",
    "plt.colorbar(scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the plot of k-means clustering, we see that there are some distinct clusters\n",
    "separated by different colors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.3'></a>\n",
    "## 5.3. Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = AffinityPropagation()\n",
    "ap.fit(X)\n",
    "clust_labels2 = ap.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(X.iloc[:,0],X.iloc[:,1], c =clust_labels2, cmap =\"rainbow\")\n",
    "ax.set_title('Affinity')\n",
    "ax.set_xlabel('Mean Return')\n",
    "ax.set_ylabel('Volatility')\n",
    "plt.colorbar(scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the plot of k-means clustering, we see that there are some distinct clusters separated by different colors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.3.1'></a>\n",
    "### 5.3.1 Cluster Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_indices = ap.cluster_centers_indices_\n",
    "labels = ap.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_clusters = len(cluster_centers_indices)\n",
    "print('Estimated number of clusters: %d' % no_clusters)\n",
    "# Plot exemplars\n",
    "\n",
    "X_temp=np.asarray(X)\n",
    "plt.close('all')\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(no_clusters), colors):\n",
    "    class_members = labels == k\n",
    "    cluster_center = X_temp[cluster_centers_indices[k]]\n",
    "    plt.plot(X_temp[class_members, 0], X_temp[class_members, 1], col + '.')\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=14)\n",
    "    for x in X_temp[class_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show number of stocks in each cluster\n",
    "clustered_series_ap = pd.Series(index=X.index, data=ap.labels_.flatten())\n",
    "# clustered stock with its cluster label\n",
    "clustered_series_all_ap = pd.Series(index=X.index, data=ap.labels_.flatten())\n",
    "clustered_series_ap = clustered_series_ap[clustered_series != -1]\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.barh(\n",
    "    range(len(clustered_series_ap.value_counts())), # cluster labels, y axis\n",
    "    clustered_series_ap.value_counts()\n",
    ")\n",
    "plt.title('Cluster Member Counts')\n",
    "plt.xlabel('Stocks in Cluster')\n",
    "plt.ylabel('Cluster Number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.4'></a>\n",
    "## 5.4. Cluster Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient (sklearn.metrics.silhouette_score) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(\"km\", metrics.silhouette_score(X, k_means.labels_, metric='euclidean'))\n",
    "print(\"hc\", metrics.silhouette_score(X, hc.fit_predict(X), metric='euclidean'))\n",
    "print(\"ap\", metrics.silhouette_score(X, ap.labels_, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the affinity propagation performs the best, we go ahead with the affinity propagation and use the 5 clusters as specified by this clustering method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the return within a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The understand the intuition behind clustering, let us visualize the results of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all stock with its cluster label (including -1)\n",
    "clustered_series = pd.Series(index=X.index, data=ap.fit_predict(X).flatten())\n",
    "# clustered stock with its cluster label\n",
    "clustered_series_all = pd.Series(index=X.index, data=ap.fit_predict(X).flatten())\n",
    "clustered_series = clustered_series[clustered_series != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of stocks in each cluster\n",
    "counts = clustered_series_ap.value_counts()\n",
    "\n",
    "# let's visualize some clusters\n",
    "cluster_vis_list = list(counts[(counts<25) & (counts>1)].index)[::-1]\n",
    "cluster_vis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_SIZE_LIMIT = 9999\n",
    "counts = clustered_series.value_counts()\n",
    "ticker_count_reduced = counts[(counts>1) & (counts<=CLUSTER_SIZE_LIMIT)]\n",
    "print (\"Clusters formed: %d\" % len(ticker_count_reduced))\n",
    "print (\"Pairs to evaluate: %d\" % (ticker_count_reduced*(ticker_count_reduced-1)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "cutoff_date = dt.date(2023,1,1)\n",
    "\n",
    "\n",
    "for clust in cluster_vis_list:\n",
    "    tickers = list(clustered_series[clustered_series==clust].index)\n",
    "    means = np.log(dataset.loc[:cutoff_date, tickers].mean())\n",
    "    data = np.log(dataset.loc[:cutoff_date, tickers]).sub(means)\n",
    "    data.plot(title='Stock Time Series for Cluster %d' % clust)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the charts above, across all the clusters with small number of stocks, we\n",
    "see similar movement of the stocks under different clusters, which corroborates the\n",
    "effectiveness of the clustering technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "# 6. Pairs Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.1'></a>\n",
    "## 6.1. Cointegration and Pair Selection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cointegrated_pairs(data, significance=0.05):\n",
    "    # This function is from https://www.quantopian.com/lectures/introduction-to-pairs-trading\n",
    "    n = data.shape[1]    \n",
    "    score_matrix = np.zeros((n, n))\n",
    "    pvalue_matrix = np.ones((n, n))\n",
    "    keys = data.keys()\n",
    "    pairs = []\n",
    "    for i in range(1):\n",
    "        for j in range(i+1, n):\n",
    "            S1 = data[keys[i]]            \n",
    "            S2 = data[keys[j]]\n",
    "            result = coint(S1, S2)\n",
    "            score = result[0]\n",
    "            pvalue = result[1]\n",
    "            score_matrix[i, j] = score\n",
    "            pvalue_matrix[i, j] = pvalue\n",
    "            if pvalue < significance:\n",
    "                pairs.append((keys[i], keys[j]))\n",
    "    return score_matrix, pvalue_matrix, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import coint\n",
    "cluster_dict = {}\n",
    "for i, which_clust in enumerate(ticker_count_reduced.index):\n",
    "    tickers = clustered_series[clustered_series == which_clust].index   \n",
    "    score_matrix, pvalue_matrix, pairs = find_cointegrated_pairs(\n",
    "        dataset[tickers]\n",
    "    )\n",
    "    cluster_dict[which_clust] = {}\n",
    "    cluster_dict[which_clust]['score_matrix'] = score_matrix\n",
    "    cluster_dict[which_clust]['pvalue_matrix'] = pvalue_matrix\n",
    "    cluster_dict[which_clust]['pairs'] = pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for clust in cluster_dict.keys():\n",
    "    pairs.extend(cluster_dict[clust]['pairs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Number of pairs found : %d\" % len(pairs))\n",
    "print (\"In those pairs, there are %d unique tickers.\" % len(np.unique(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.2'></a>\n",
    "## 6.2. Pair Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.cm as cm\n",
    "stocks = np.unique(pairs)\n",
    "X_df = pd.DataFrame(index=X.index, data=X).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_pairs_series = clustered_series.loc[stocks]\n",
    "stocks = list(np.unique(pairs))\n",
    "X_pairs = X_df.T.loc[stocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne = TSNE(learning_rate=50, perplexity=1, random_state=1337).fit_transform(X_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, facecolor='white',figsize=(16,8))\n",
    "plt.clf()\n",
    "plt.axis('off')\n",
    "for pair in pairs:\n",
    "    #print(pair[0])\n",
    "    ticker1 = pair[0]\n",
    "    loc1 = X_pairs.index.get_loc(pair[0])\n",
    "    x1, y1 = X_tsne[loc1, :]\n",
    "    #print(ticker1, loc1)\n",
    "\n",
    "    ticker2 = pair[0]\n",
    "    loc2 = X_pairs.index.get_loc(pair[1])\n",
    "    x2, y2 = X_tsne[loc2, :]\n",
    "      \n",
    "    plt.plot([x1, x2], [y1, y2], 'k-', alpha=0.3, c='gray');\n",
    "    \n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=220, alpha=0.9, c=in_pairs_series.values, cmap=cm.Paired)\n",
    "plt.title('T-SNE Visualization of Validated Pairs'); \n",
    "\n",
    "# zip joins x and y coordinates in pairs\n",
    "for x,y,name in zip(X_tsne[:,0],X_tsne[:,1],X_pairs.index):\n",
    "\n",
    "    label = name\n",
    "\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "    \n",
    "plt.plot(centroids[:,0],centroids[:,1],'sg',markersize=11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "# 7. Implement a Pairs Trading Methodology \n",
    "\n",
    "Now that the clustering algorithm has identified a possible pairs trading opportunity, we can implement the pairs-trading methodology implemented [here](https://github.com/williamzjasmine/CUNY_SPS_DS/blob/master/DATA_618/Major_Assignments/Pairs_Trading/Portfolio_Pairs_Trading_Opportunities.ipynb) in order to see if we can generate any profit. The cell below determines all the instances over the past three years in which we should have initiated a trade, what kind of trade it should be, and when to close out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_threshold = 1.5\n",
    "df = dataset.reset_index()[['Date', 'NEE', 'PFE']]\n",
    "\n",
    "df.columns = ['Date', 'A1', 'A2']\n",
    "#df = df[df['Date'] >= dt.date(2022, 1, 1)].reset_index()\n",
    "n = np.log(df['A1'].iloc[0]) / np.log(df['A2'].iloc[0])\n",
    "\n",
    "df['S'] = np.log(df['A1']) - n * np.log(df['A2'])\n",
    "df['S_mean'] = df['S'].expanding().mean()\n",
    "df['S_std'] = df['S'].expanding().std()\n",
    "df['z'] = (df['S'] - df['S_mean']) / df['S_std']\n",
    "\n",
    "breaks = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    date = row['Date']\n",
    "    z = row['z']\n",
    "    A2 = row['A2']\n",
    "    A1 = row['A1']\n",
    "    mean = row['S_mean']\n",
    "    \n",
    "    if z > z_threshold and df.iloc[index-1]['z'] <= z_threshold:\n",
    "        print(f'Indicator broken at {date}: z = {z}')\n",
    "        tmp = df.iloc[index:]\n",
    "        if mean > 0:\n",
    "            tmp = tmp[tmp['S_mean'] < 0]\n",
    "            if tmp.shape[0] > 0:\n",
    "                tmp = tmp.iloc[0]\n",
    "                breaks.append([date, tmp['Date'], A1, A2, 'shortlong', tmp['A1'], tmp['A2']])\n",
    "            else:\n",
    "                breaks.append([date, None, A1, A2, 'shortlong', None, None])\n",
    "        else:\n",
    "            tmp = tmp[tmp['S_mean'] > 0]\n",
    "            if tmp.shape[0] > 0:\n",
    "                tmp = tmp.iloc[0]\n",
    "                breaks.append([date, tmp['Date'], A1, A2, 'shortlong', tmp['A1'], tmp['A2']])\n",
    "            else:\n",
    "                breaks.append([date, None, A1, A2, 'shortlong', None, None])\n",
    "            \n",
    "    if z < -z_threshold and df.iloc[index-1]['z'] >= -z_threshold:\n",
    "        print(f'Indicator broken at {date}: z = {z}')\n",
    "        tmp = df.iloc[index:]\n",
    "        if mean > 0:\n",
    "            tmp = tmp[tmp['S_mean'] < 0] \n",
    "            if tmp.shape[0] > 0:\n",
    "                tmp = tmp.iloc[0]\n",
    "                breaks.append([date, tmp['Date'], A1, A2, 'longshort', tmp['A1'], tmp['A2']])\n",
    "            else:\n",
    "                breaks.append([date, None, A1, A2, 'longshort', None, None])\n",
    "        else:\n",
    "            tmp = tmp[tmp['S_mean'] > 0]\n",
    "            if tmp.shape[0] > 0:\n",
    "                tmp = tmp.iloc[0]\n",
    "                breaks.append([date, tmp['Date'], A1, A2, 'shortlong', tmp['A1'], tmp['A2']])\n",
    "            else:\n",
    "                breaks.append([date, None, A1, A2, 'shortlong', None, None])\n",
    "        \n",
    "breaks = pd.DataFrame(breaks, columns=['start_date', 'end_date', 'A1i', 'A2i', 'type', 'A1f', 'A2f'])\n",
    "breaks.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A1` in this case corresponds to `NEE` stock, while `A2` corresponds to `PFE`. The function below can be used to evaluate proposed trades above to determine if they result in any profit in the long term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_profit(p1i, p1f, p2i, p2f, trade_type, trade_amount):\n",
    "    money_spent = 0\n",
    "    money_made = 0\n",
    "    p2_buy = 0\n",
    "    p1_borrow = 0\n",
    "    p2_borrow = 0 \n",
    "    p1_buy = 0\n",
    "    \n",
    "    if trade_type == 'shortlong':\n",
    "        money_spent += trade_amount\n",
    "        money_made += trade_amount\n",
    "        p2_buy = trade_amount / p2i\n",
    "        p1_borrow = trade_amount / p1i\n",
    "        \n",
    "        money_spent += p1_borrow * p1f\n",
    "        money_made += p2_buy * p2f\n",
    "        return((money_spent, money_made))\n",
    "    \n",
    "    elif trade_type == 'longshort':\n",
    "        money_spent += trade_amount\n",
    "        money_made += trade_amount\n",
    "        p1_buy = trade_amount / p1i\n",
    "        p2_borrow = trade_amount / p2i\n",
    "        \n",
    "        money_spent += p2_borrow * p2f\n",
    "        money_made += p1_buy * p1f\n",
    "        return((money_spent, money_made))\n",
    "    else:\n",
    "        return 'invalid trade type'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below uses this function on the proposed trades shown above by taking hypothetical $100 short and long positions for the instructed asset of each trade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "money_spent = 0\n",
    "money_made = 0 \n",
    "valid_trades = breaks.dropna()\n",
    "\n",
    "for index, row in valid_trades.iterrows():\n",
    "    ms, mm = calc_profit(row['A1i'], row['A1f'], row['A2i'], row['A2f'], row['type'], 100)\n",
    "    money_spent += ms\n",
    "    money_made += mm\n",
    "profit = money_made - money_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Money made: ${money_made:,.2f}')\n",
    "print(f'Money spent: ${money_spent:,.2f}')\n",
    "print(f'Profit: ${profit:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that the pairs trading strategy determined above would have resulted in a net gain of ~$120 dollars, which is about 4\\% of the amount used to make all the trades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "# Conclusion\n",
    "\n",
    "\n",
    "The clustering techniques do not directly help in stock trend prediction. However, they can be effectively used in portfolio construction for finding the right pairs, which eventually help in risk mitigation and one can achieve superior risk adjusted returns.\n",
    "\n",
    "In this case, the clustering algorithm identified a pair of stocks (NEE and PFE) that would have generated a small profit after the implementation of a rudimentary pairs-trading methodology. While the net profits of 4% isn't huge, it is much more significant when compared to the results shown [here](http://localhost:8888/notebooks/Pairs_Trading/Portfolio_Pairs_Trading_Opportunities.ipynb) when possible pairs of the same [portfolio](https://docs.google.com/spreadsheets/d/1EZj5M7dXGy-48i0PydZQa5gpUOYCQHDFRBlp_rU1sdo/edit#gid=1736594093) were identified via more \"human\" means (which never saw profits greater than 1%). Thus, we can conclude that the clustering algorithm did indeed assist in finding a more reasonable pair of assets for pairs-trading. Furthermore, the amount of money profited from the resultant pair would likely increase, should the pairs-trading methodology be improved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 206,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
